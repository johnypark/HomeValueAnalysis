{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering search term number 1 out of 8\n",
      "1 pages of listings found\n",
      "7 home listings scraped\n",
      "***\n",
      "Entering search term number 2 out of 8\n",
      "3 pages of listings found\n",
      "73 home listings scraped\n",
      "***\n",
      "Entering search term number 3 out of 8\n",
      "Search 32602 returned zero results. Moving onto the next search\n",
      "***\n",
      "Entering search term number 4 out of 8\n",
      "Search 32604 returned zero results. Moving onto the next search\n",
      "***\n",
      "Entering search term number 5 out of 8\n",
      "7 pages of listings found\n",
      "171 home listings scraped\n",
      "***\n",
      "Entering search term number 6 out of 8\n",
      "7 pages of listings found\n",
      "168 home listings scraped\n",
      "***\n",
      "Entering search term number 7 out of 8\n",
      "7 pages of listings found\n",
      "166 home listings scraped\n",
      "***\n",
      "Entering search term number 8 out of 8\n",
      "14 pages of listings found\n",
      "344 home listings scraped\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import zillow_functions as zl\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "st = zl.zipcodes_list(st_items = ['32603', '32601','32602','32604','32605','32606','32607','32608'])\n",
    "\n",
    "# Initialize the webdriver.\n",
    "driver = zl.init_driver('/usr/local/bin/chromedriver')\n",
    "\n",
    "# Go to www.zillow.com/homes\n",
    "zl.navigate_to_website(driver, \"http://www.zillow.com/homes\")\n",
    "\n",
    "# Click the \"buy\" button.\n",
    "zl.click_buy_button(driver)\n",
    "\n",
    "# Create 11 variables from the scrapped HTML data.\n",
    "# These variables will make up the final output dataframe.\n",
    "df = pd.DataFrame({'address' : [], \n",
    "                   'bathrooms' : [], \n",
    "                   'bedrooms' : [], \n",
    "                   'city' : [], \n",
    "                   'days_on_zillow' : [], \n",
    "                   'price' : [], \n",
    "                   'sale_type' : [], \n",
    "                   'state' : [], \n",
    "                   'sqft' : [], \n",
    "                   'url' : [], \n",
    "                   'zip' : []})\n",
    "\n",
    "# Get total number of search terms.\n",
    "num_search_terms = len(st)\n",
    "\n",
    "# Start the scraping.\n",
    "for k in range(num_search_terms):\n",
    "    # Define search term (must be str object).\n",
    "    search_term = st[k]\n",
    "\n",
    "    # Enter search term and execute search.\n",
    "    if zl.enter_search_term(driver, search_term):\n",
    "        print(\"Entering search term number \" + str(k+1) + \n",
    "              \" out of \" + str(num_search_terms))\n",
    "    else:\n",
    "        print(\"Search term \" + str(k+1) + \n",
    "              \" failed, moving onto next search term\\n***\")\n",
    "        continue\n",
    "    \n",
    "    # Check to see if any results were returned from the search.\n",
    "    # If there were none, move onto the next search.\n",
    "    if zl.results_test(driver):\n",
    "        print(\"Search \" + str(search_term) + \n",
    "              \" returned zero results. Moving onto the next search\\n***\")\n",
    "        continue\n",
    "    \n",
    "    # Pull the html for each page of search results. Zillow caps results at \n",
    "    # 20 pages, each page can contain 26 home listings, thus the cap on home \n",
    "    # listings per search is 520.\n",
    "    raw_data = zl.get_html(driver)\n",
    "    print(str(len(raw_data)) + \" pages of listings found\")\n",
    "    \n",
    "    # Take the extracted HTML and split it up by individual home listings.\n",
    "    listings = zl.get_listings(raw_data)\n",
    "    \n",
    "    # For each home listing, extract the 11 variables that will populate that \n",
    "    # specific observation within the output dataframe.\n",
    "    for n in range(len(listings)):\n",
    "        soup = BeautifulSoup(listings[n], \"lxml\")\n",
    "        new_obs = []\n",
    "        \n",
    "        # List that contains number of beds, baths, and total sqft (and \n",
    "        # sometimes price as well).\n",
    "        card_info = zl.get_card_info(soup)\n",
    "        \n",
    "        # Street Address\n",
    "        new_obs.append(zl.get_street_address(soup))\n",
    "        \n",
    "        # Bathrooms\n",
    "        new_obs.append(zl.get_bathrooms(card_info))\n",
    "        \n",
    "        # Bedrooms\n",
    "        new_obs.append(zl.get_bedrooms(card_info))\n",
    "        \n",
    "        # City\n",
    "        new_obs.append(zl.get_city(soup))\n",
    "        \n",
    "        # Days on the Market/Zillow\n",
    "        new_obs.append(zl.get_days_on_market(soup))\n",
    "        \n",
    "        # Price\n",
    "        new_obs.append(zl.get_price(soup, card_info))\n",
    "        \n",
    "        # Sale Type (House for Sale, New Construction, Foreclosure, etc.)\n",
    "        new_obs.append(zl.get_sale_type(soup))\n",
    "        \n",
    "        # Sqft\n",
    "        new_obs.append(zl.get_sqft(card_info))\n",
    "        \n",
    "        # State\n",
    "        new_obs.append(zl.get_state(soup))\n",
    "        \n",
    "        # URL for each house listing\n",
    "        new_obs.append(zl.get_url(soup))\n",
    "        \n",
    "        # Zipcode\n",
    "        new_obs.append(zl.get_zipcode(soup))\n",
    "    \n",
    "        # Append new_obs to df as a new observation\n",
    "        if len(new_obs) == len(df.columns):\n",
    "            df.loc[len(df.index)] = new_obs\n",
    "\n",
    "# Close the webdriver connection.\n",
    "zl.close_connection(driver)\n",
    "\n",
    "# Write df to CSV.\n",
    "columns = ['address', 'city', 'state', 'zip', 'price', 'sqft', 'bedrooms', \n",
    "           'bathrooms', 'days_on_zillow', 'sale_type', 'url']\n",
    "df = df[columns]\n",
    "dt = time.strftime(\"%Y-%m-%d\") + \"_\" + time.strftime(\"%H%M%S\")\n",
    "file_name = str(dt) + \".csv\"\n",
    "df.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
